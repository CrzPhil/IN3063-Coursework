{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CrzPhil/IN3063-Coursework/blob/main/Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfd1ebf",
      "metadata": {
        "id": "edfd1ebf"
      },
      "source": [
        "# IN3063 - Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da372ae8",
      "metadata": {
        "id": "da372ae8"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "723a7cf4",
      "metadata": {
        "id": "723a7cf4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb0f9e5",
      "metadata": {
        "id": "8cb0f9e5"
      },
      "source": [
        "## Sigmoid & ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412fa30c",
      "metadata": {
        "id": "412fa30c"
      },
      "source": [
        "- By Aymen\n",
        "- Reference:\n",
        "    - https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
        "    - https://www.sharpsightlabs.com/blog/numpy-relu/\n",
        "    - Lab 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ed98cad4",
      "metadata": {
        "id": "ed98cad4"
      },
      "outputs": [],
      "source": [
        "# Forward pass for Sigmoid\n",
        "def forward_sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Backward pass for Sigmoid\n",
        "def backward_sigmoid(x):\n",
        "    return forward_sigmoid(x) * (1 - forward_sigmoid(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ed656454",
      "metadata": {
        "id": "ed656454"
      },
      "outputs": [],
      "source": [
        "# Forward pass for ReLU\n",
        "def forward_relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Backward pass for ReLU\n",
        "def backward_relu(x):\n",
        "    return np.where(x > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f493982",
      "metadata": {
        "id": "7f493982"
      },
      "source": [
        "## Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febca3b9",
      "metadata": {
        "id": "febca3b9"
      },
      "source": [
        "- By Aymen\n",
        "- Using the Numpy version\n",
        "- Reference:\n",
        "    - https://towardsdatascience.com/softmax-function-simplified-714068bf8156\n",
        "    - https://en.wikipedia.org/wiki/Softmax_function\n",
        "    - https://www.sharpsightlabs.com/blog/numpy-softmax/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6c0f1466",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c0f1466",
        "outputId": "df48b6f8-0c4b-401e-9188-54964656c653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward pass result: [0. 1. 0.]\n",
            "Backward pass result: [[0. 0. 0.]]\n",
            "\n",
            "\n",
            "Forward pass result: [0.09003057 0.24472847 0.66524096]\n",
            "Backward pass result: [[0.08192507 0.18483645 0.22269543]]\n"
          ]
        }
      ],
      "source": [
        "# Forward pass for Softmax\n",
        "def forward_softmax(x):\n",
        "    exponential = np.exp(x - np.max(x))\n",
        "    return exponential / exponential.sum() # calculates softmax probability\n",
        "\n",
        "# Backward pass for Softmax\n",
        "def backward_softmax(x):\n",
        "    return np.reshape(forward_softmax(x) * (1 - forward_softmax(x)), (1, -1)) # computes gradient of softmax\n",
        "\n",
        "# Testing:\n",
        "x = np.array([100.0, 2000.0, 300.0]) # large numbers\n",
        "print(\"Forward pass result:\", forward_softmax(x))\n",
        "print(\"Backward pass result:\", backward_softmax(x))\n",
        "print (\"\\n\")\n",
        "\n",
        "x = np.array([1.0, 2.0, 3.0]) # small numbers\n",
        "print(\"Forward pass result:\", forward_softmax(x))\n",
        "print(\"Backward pass result:\", backward_softmax(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31dd9266",
      "metadata": {
        "id": "31dd9266"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66744b60",
      "metadata": {
        "id": "66744b60"
      },
      "source": [
        "- By Adam\n",
        "- References\n",
        "  - Lecture 7\n",
        "  - https://stackoverflow.com/questions/70836518/typeerror-bad-operand-type-for-unary-list-python\n",
        "  - https://stackoverflow.com/questions/25854380/enforce-arguments-to-a-specific-list-of-values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3955086a",
      "metadata": {
        "id": "3955086a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01db0362-1fa8-450f-8e94-c58847343c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.88079708 0.98201379 0.99908895 0.99966465]\n",
            "[0.44039854 0.4910069  0.49954447 0.49983232]\n",
            "[1.76159416 1.96402758 1.9981779  1.9993293 ]\n",
            "[0.         1.96402758 1.9981779  1.9993293 ]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Valid value structure constants.\n",
        "They're defined here so they aren't recreated every time the function is ran.\n",
        "'''\n",
        "ACTIVATION_FUNCTIONS = {\n",
        "    \"sigmoid\": [forward_sigmoid, backward_sigmoid],\n",
        "    \"relu\": [forward_relu, backward_relu],\n",
        "    \"softmax\": [forward_softmax, backward_softmax]\n",
        "}\n",
        "\n",
        "VALID_DIRECTIONS = [\"forward\", \"backward\"]\n",
        "\n",
        "'''\n",
        "Dropout function\n",
        "    x = input vector\n",
        "    probability is a float between 0.0 and 1.0\n",
        "    activation_function is a string that corresponds to one of the key values above\n",
        "        determines which activation function to use\n",
        "    direction is a string that corresponds to one of the array values above\n",
        "        determines whether to use a forward or backward pass activation function\n",
        "    inverted is a boolean\n",
        "        determines whether or not use inverted dropout\n",
        "    train is a boolean\n",
        "        determines whether to train or test\n",
        "'''\n",
        "def dropout(x, probability, activation_function, direction, inverted, train):\n",
        "    if activation_function not in ACTIVATION_FUNCTIONS.keys():\n",
        "        raise ValueError(f\"Activation function must be one of {ACTIVATION_FUNCTIONS.keys()}\")\n",
        "\n",
        "    if direction not in VALID_DIRECTIONS:\n",
        "        raise ValueError(f\"Direction must be one of {VALID_DIRECTIONS}\")\n",
        "\n",
        "    value_index = 0 if direction == \"forward\" else 1\n",
        "\n",
        "    H1 = ACTIVATION_FUNCTIONS[activation_function][value_index](x)\n",
        "    mask = (np.random.rand(*H1.shape) < probability)\n",
        "\n",
        "    if inverted:\n",
        "        return H1 * (mask / probability) if train else H1\n",
        "    else:\n",
        "        return H1 * mask if train else H1 * probability\n",
        "\n",
        "# Testing the function\n",
        "#     Starting by defining x\n",
        "x = np.array([2.0, 4.0, 7.0, 8.0])\n",
        "\n",
        "#     Training\n",
        "H1_dropped = dropout(x, 0.5, \"sigmoid\", \"forward\", False, True)\n",
        "print(H1_dropped)\n",
        "\n",
        "#     Testing\n",
        "H1_dropped = dropout(x, 0.5, \"sigmoid\", \"forward\", False, False)\n",
        "print(H1_dropped)\n",
        "\n",
        "#     Training, inverted\n",
        "H1_dropped = dropout(x, 0.5, \"sigmoid\", \"forward\", True, True)\n",
        "print(H1_dropped)\n",
        "\n",
        "#     Testing, inverted\n",
        "H1_dropped = dropout(x, 0.5, \"sigmoid\", \"forward\", True, True)\n",
        "print(H1_dropped)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875f38de",
      "metadata": {
        "id": "875f38de"
      },
      "source": [
        "## Neural Network\n",
        "- By Philip  \n",
        "References:\n",
        "  - Lecture 6\n",
        "  - Lecture 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3bccdc1a",
      "metadata": {
        "id": "3bccdc1a"
      },
      "outputs": [],
      "source": [
        "# Reading the MNIST dataset as per http://yann.lecun.com/exdb/mnist/\n",
        "import os\n",
        "import struct\n",
        "\n",
        "def read_idx(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        # Read two bytes (big endian and unsigned)\n",
        "        zero, data_type, dims = struct.unpack('>HBB', file.read(4))\n",
        "        # Four byte integer big endian\n",
        "        shape = tuple(struct.unpack('>I', file.read(4))[0] for d in range(dims))\n",
        "        return np.frombuffer(file.read(), dtype=np.uint8).reshape(shape)\n",
        "\n",
        "def load_mnist(path):\n",
        "    # Paths to the files\n",
        "    train_images_path = os.path.join(path, 'train-images-idx3-ubyte')\n",
        "    train_labels_path = os.path.join(path, 'train-labels-idx1-ubyte')\n",
        "    test_images_path = os.path.join(path, 't10k-images-idx3-ubyte')\n",
        "    test_labels_path = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
        "\n",
        "    # Loading the datasets\n",
        "    train_images = read_idx(train_images_path)\n",
        "    train_labels = read_idx(train_labels_path)\n",
        "    test_images = read_idx(test_images_path)\n",
        "    test_labels = read_idx(test_labels_path)\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ccca774e",
      "metadata": {
        "id": "ccca774e"
      },
      "outputs": [],
      "source": [
        "# Example use\n",
        "t_images, t_labels, test_images, test_labels = load_mnist('./dataset')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_images[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPf3yf_59d9Q",
        "outputId": "315eaf6f-2bb6-4fa2-92dc-f9000930dd1d"
      },
      "id": "VPf3yf_59d9Q",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe77577b",
      "metadata": {
        "id": "fe77577b"
      },
      "outputs": [],
      "source": [
        "class NeuralNet:\n",
        "  def __init__(self, activation_function, layers: int, batch_size: int, neurons: list):\n",
        "    \"\"\"\n",
        "    Initialises a new instance of the NeuralNet class.\n",
        "\n",
        "    Parameters:\n",
        "    activation_function (func): The activation function to be used in the network layers.\n",
        "                                The function is used in all layers.\n",
        "    layers (int): The number of layers in the neural network.\n",
        "    batch_size (int): The size of the batches used in training. This affects how the data is split during training iterations.\n",
        "    neurons (list of int): The number of neurons in each layer. This should be a list where each element represents\n",
        "                            the number of neurons in the respective layer of the network.\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    self.activation_function = activation_function\n",
        "    self.layers = layers\n",
        "    self.batch_size = batch_size\n",
        "    self.neurons = neurons\n",
        "    # Will be initialised once features are known\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "\n",
        "  def init_weights_and_biases(self, input_features):\n",
        "    # Initialise weights and biases based on the layers, neurons, and input features\n",
        "    # Fully connected through weights\n",
        "    for i in range(self.layers):\n",
        "      if i == 0:\n",
        "          layer_weights = np.random.randn(self.neurons[i], input_features) * 0.01\n",
        "      else:\n",
        "          layer_weights = np.random.randn(self.neurons[i], self.neurons[i - 1]) * 0.01\n",
        "      layer_bias = np.zeros((self.neurons[i], 1))\n",
        "      self.weights.append(layer_weights)\n",
        "      self.biases.append(layer_bias)\n",
        "\n",
        "  def forward_pass(self, X):\n",
        "    activations = [X]\n",
        "    for i in range(self.layers):\n",
        "      Z = np.dot(self.weights[i], activations[-1]) + self.biases[i]\n",
        "      A = self.apply_activation(Z)\n",
        "      activations.append(A)\n",
        "    return activations\n",
        "\n",
        "  def apply_activation(self, Z):\n",
        "    return self.activation_function(Z)\n",
        "\n",
        "  def backward_pass(self, Y, activations):\n",
        "    m = Y.shape[1]\n",
        "    n = len(self.weights)\n",
        "    gradients = {}\n",
        "\n",
        "    # Output layer\n",
        "    dA = activations[-1] - Y  # Derivative of loss wrt (with respect to) output\n",
        "\n",
        "    for i in reversed(range(n)):\n",
        "      dZ = dA * self.activation_function[1](activations[i+1])\n",
        "      dW = np.dot(dZ, activations[i].T) / m\n",
        "      db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "      if i > 0:\n",
        "        dA = np.dot(self.weights[i].T, dZ)\n",
        "\n",
        "      gradients['dW' + str(i + 1)] = dW\n",
        "      gradients['db' + str(i + 1)] = db\n",
        "\n",
        "    return gradients\n",
        "\n",
        "  def update_weights_and_biases(self, gradients, learning_rate):\n",
        "    for i in range(self.layers):\n",
        "      pass\n",
        "\n",
        "  def train_network(self, epochs, batch_size, learning_rate, X_train, Y_train):\n",
        "    # Initialise weights & biases\n",
        "    self.init_weights_and_biases(X_train)\n",
        "    loss_across_epochs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      # Maybe shuffle training data before batching it?\n",
        "\n",
        "      # Iterate batches\n",
        "      for i in range(0, X_train.shape[0], batch_size):\n",
        "        X_batch = X_train[i:i + batch_size]\n",
        "        Y_batch = Y_train[i:i + batch_size]\n",
        "\n",
        "        # Forward pass over the batch\n",
        "        activations = self.forward_pass(X_batch)\n",
        "\n",
        "        # Backward pass over the batch (get gradients)\n",
        "        gradients = self.backward_pass(activations, Y_batch)\n",
        "\n",
        "        # Update weights & biases\n",
        "        self.update_weights_and_biases(gradients, learning_rate)\n",
        "\n",
        "\n",
        "  def evaluate_model(self, x_test, y_test, X_train, Y_train, loss_list):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Gza9AC4OGU"
      },
      "id": "6-Gza9AC4OGU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CrzPhil/IN3063-Coursework/blob/main/Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfd1ebf",
      "metadata": {
        "id": "edfd1ebf"
      },
      "source": [
        "# IN3063 - Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da372ae8",
      "metadata": {
        "id": "da372ae8"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "723a7cf4",
      "metadata": {
        "id": "723a7cf4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb0f9e5",
      "metadata": {
        "id": "8cb0f9e5"
      },
      "source": [
        "## Sigmoid & ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412fa30c",
      "metadata": {
        "id": "412fa30c"
      },
      "source": [
        "- By Aymen\n",
        "- Reference:\n",
        "    - https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
        "    - https://www.sharpsightlabs.com/blog/numpy-relu/\n",
        "    - Lab 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed98cad4",
      "metadata": {
        "id": "ed98cad4"
      },
      "outputs": [],
      "source": [
        "# Forward pass for Sigmoid\n",
        "def forward_sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Backward pass for Sigmoid\n",
        "def backward_sigmoid(x):\n",
        "    return forward_sigmoid(x) * (1 - forward_sigmoid(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed656454",
      "metadata": {
        "id": "ed656454"
      },
      "outputs": [],
      "source": [
        "# Forward pass for ReLU\n",
        "def forward_relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Backward pass for ReLU\n",
        "def backward_relu(x):\n",
        "    return np.where(x > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f493982",
      "metadata": {
        "id": "7f493982"
      },
      "source": [
        "## Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febca3b9",
      "metadata": {
        "id": "febca3b9"
      },
      "source": [
        "- By Aymen\n",
        "- Using the Numpy version\n",
        "- Reference:\n",
        "    - https://towardsdatascience.com/softmax-function-simplified-714068bf8156\n",
        "    - https://en.wikipedia.org/wiki/Softmax_function\n",
        "    - https://www.sharpsightlabs.com/blog/numpy-softmax/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0f1466",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c0f1466",
        "outputId": "74951742-018a-4122-80e3-979c03957203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward pass result: [0. 1. 0.]\n",
            "Backward pass result: [[0. 0. 0.]]\n",
            "\n",
            "\n",
            "Forward pass result: [0.09003057 0.24472847 0.66524096]\n",
            "Backward pass result: [[0.08192507 0.18483645 0.22269543]]\n"
          ]
        }
      ],
      "source": [
        "# Forward pass for Softmax\n",
        "def forward_softmax(x):\n",
        "    exponential = np.exp(x - np.max(x))\n",
        "    return exponential / exponential.sum() # calculates softmax probability\n",
        "\n",
        "# Backward pass for Softmax\n",
        "def backward_softmax(x):\n",
        "    return np.reshape(forward_softmax(x) * (1 - forward_softmax(x)), (1, -1)) # computes gradient of softmax\n",
        "\n",
        "# Testing:\n",
        "x = np.array([100.0, 2000.0, 300.0]) # large numbers\n",
        "print(\"Forward pass result:\", forward_softmax(x))\n",
        "print(\"Backward pass result:\", backward_softmax(x))\n",
        "print (\"\\n\")\n",
        "\n",
        "x = np.array([1.0, 2.0, 3.0]) # small numbers\n",
        "print(\"Forward pass result:\", forward_softmax(x))\n",
        "print(\"Backward pass result:\", backward_softmax(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31dd9266",
      "metadata": {
        "id": "31dd9266"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66744b60",
      "metadata": {
        "id": "66744b60"
      },
      "source": [
        "- By Adam\n",
        "- References\n",
        "  - Lecture 7\n",
        "  - https://stackoverflow.com/questions/70836518/typeerror-bad-operand-type-for-unary-list-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3955086a",
      "metadata": {
        "id": "3955086a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf291462-2750-4309-c763-cf0f6d493e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.76159416]\n",
            "[0.88079708]\n"
          ]
        }
      ],
      "source": [
        "def dropout(x, probability, activation_function, train): # x = input vector, probability is a float, activation_function is a string, train is a boolean\n",
        "    activation_functions = {\n",
        "        \"sigmoid\": forward_sigmoid,\n",
        "        \"relu\": forward_relu,\n",
        "        \"softmax\": forward_softmax\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        H1 = activation_functions[activation_function](x)\n",
        "    except KeyError:\n",
        "        print(\"Invalid activation function passed\")\n",
        "        return -1\n",
        "\n",
        "    if train:\n",
        "        mask = (np.random.rand(*H1.shape) < probability) / probability\n",
        "        return H1 * mask\n",
        "    else:\n",
        "        return H1\n",
        "\n",
        "# Testing: Training\n",
        "x = np.arange(2.0, 4.0, 7.0)\n",
        "H1_dropped = dropout(x, 0.5, \"sigmoid\", True)\n",
        "print(H1_dropped)\n",
        "\n",
        "# Testing: Testing\n",
        "H1_dropped2 = dropout(x, 0.5, \"sigmoid\", False)\n",
        "print(H1_dropped2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875f38de",
      "metadata": {
        "id": "875f38de"
      },
      "source": [
        "## Neural Network\n",
        "- By Philip  \n",
        "Implement a fully parametrizable neural network class\n",
        "You should implement a fully-connected NN class where with number of\n",
        "hidden layers, units, activation functions can be changed. In addition, you\n",
        "can add dropout or regularizer (L1 or L2). Report the parameters used\n",
        "(update rule, learning rate, decay, epochs, batch size) and include the plots\n",
        "in your report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bccdc1a",
      "metadata": {
        "id": "3bccdc1a"
      },
      "outputs": [],
      "source": [
        "# Reading the MNIST dataset as per http://yann.lecun.com/exdb/mnist/\n",
        "import os\n",
        "import struct\n",
        "\n",
        "def read_idx(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        # Read two bytes (big endian and unsigned)\n",
        "        zero, data_type, dims = struct.unpack('>HBB', file.read(4))\n",
        "        # Four byte integer big endian\n",
        "        shape = tuple(struct.unpack('>I', file.read(4))[0] for d in range(dims))\n",
        "        return np.frombuffer(file.read(), dtype=np.uint8).reshape(shape)\n",
        "\n",
        "def load_mnist(path):\n",
        "    # Paths to the files\n",
        "    train_images_path = os.path.join(path, 'train-images-idx3-ubyte')\n",
        "    train_labels_path = os.path.join(path, 'train-labels-idx1-ubyte')\n",
        "    test_images_path = os.path.join(path, 't10k-images-idx3-ubyte')\n",
        "    test_labels_path = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
        "\n",
        "    # Loading the datasets\n",
        "    train_images = read_idx(train_images_path)\n",
        "    train_labels = read_idx(train_labels_path)\n",
        "    test_images = read_idx(test_images_path)\n",
        "    test_labels = read_idx(test_labels_path)\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccca774e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "ccca774e",
        "outputId": "cf74b575-0a63-43b7-fb3f-846a4e2c5769"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c769e20d4a12>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_mnist' is not defined"
          ]
        }
      ],
      "source": [
        "# Example use\n",
        "t_images, t_labels, test_images, test_labels = load_mnist('./dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe77577b",
      "metadata": {
        "id": "fe77577b"
      },
      "outputs": [],
      "source": [
        "class NeuralNet:\n",
        "  def __init__(self, activation_function, layers, batch_size, neurons):\n",
        "    \"\"\"\n",
        "    Initialises a new instance of the NeuralNet class.\n",
        "\n",
        "    Parameters:\n",
        "    activation_function (func): The activation function to be used in the network layers.\n",
        "                                The function is used in all layers.\n",
        "    layers (int): The number of layers in the neural network.\n",
        "    batch_size (int): The size of the batches used in training. This affects how the data is split during training iterations.\n",
        "    neurons (list of int): The number of neurons in each layer. This should be a list where each element represents\n",
        "                            the number of neurons in the respective layer of the network.\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    self.activation_function = activation_function\n",
        "    self.layers = layers\n",
        "    self.batch_size = batch_size\n",
        "    self.neurons = neurons\n",
        "    # Will be initialised once features are known\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "\n",
        "  def init_weights_and_biases(self, input_features):\n",
        "    # Initialise weights and biases based on the layers, neurons, and input features\n",
        "    # Fully connected through weights\n",
        "    for i in range(self.layers):\n",
        "      if i == 0:\n",
        "          layer_weights = np.random.randn(self.neurons[i], input_features) * 0.01\n",
        "      else:\n",
        "          layer_weights = np.random.randn(self.neurons[i], self.neurons[i - 1]) * 0.01\n",
        "      layer_bias = np.zeros((self.neurons[i], 1))\n",
        "      self.weights.append(layer_weights)\n",
        "      self.biases.append(layer_bias)\n",
        "\n",
        "  def forward_pass(self, X):\n",
        "    activations = [X]\n",
        "    for i in range(self.layers):\n",
        "      Z = np.dot(self.weights[i], activations[-1]) + self.biases[i]\n",
        "      A = self.apply_activation(Z)\n",
        "      activations.append(A)\n",
        "    return activations\n",
        "\n",
        "  def apply_activation(self, Z):\n",
        "    return self.activation_function(Z)\n",
        "\n",
        "  def backward_pass(self, X, Y):\n",
        "    pass\n",
        "\n",
        "  def update_weights_and_biases(self, gradients, learning_rate):\n",
        "    for i in range(self.layers):\n",
        "      pass\n",
        "\n",
        "  def train_network(self, epochs, batch_size, learning_rate, X_train, Y_train):\n",
        "    for epoch in range(epochs):\n",
        "      # Maybe shuffle training data before batching it?\n",
        "\n",
        "      # Iterate batches\n",
        "      for i in range(0, X_train.shape[0], batch_size):\n",
        "        X_batch = X_train[i:i + batch_size]\n",
        "        Y_batch = Y_train[i:i + batch_size]\n",
        "\n",
        "        # Forward pass over the batch\n",
        "        activations = self.forward_pass(X_batch)\n",
        "\n",
        "        # Backward pass over the batch (get gradients)\n",
        "        gradients = self.backward_pass(activations, Y_batch)\n",
        "\n",
        "        # Update weights & biases\n",
        "        self.update_weights_and_bies(gradients, learning_rate)\n",
        "\n",
        "\n",
        "  def evaluate_model(self, x_test, y_test, X_train, Y_train, loss_list):\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}